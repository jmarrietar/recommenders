{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/recommenders/blob/jmarrietar%2Franking/basic_ranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plan de accion:\n",
        "---------------\n",
        "\n",
        "    - 1. Sacar metricas con el utils de recommenders.\n",
        "    - 2. Intentar utilizar el load de recommenders aca. (Opcional: - aunque ayudaria pal de Pytorch tener esto dominado - Hacer otra version)\n",
        "    - 3. Cuando ya tenga esto domindado tf-recommenders migrar implementacion a Pytorch.\n",
        "    - 4. Hacer Notebook para recommenders.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Zt0QNNm3vI8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "# Recommending movies: ranking\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/basic_ranking\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/basic_ranking.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/basic_ranking.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/basic_ranking.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2jMHkZQYB5"
      },
      "source": [
        "Real-world recommender systems are often composed of two stages:\n",
        "\n",
        "1. The retrieval stage is responsible for selecting an initial set of hundreds of candidates from all possible candidates. The main objective of this model is to efficiently weed out all candidates that the user is not interested in. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
        "2. The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates.\n",
        "\n",
        "We're going to focus on the second stage, ranking. If you are interested in the retrieval stage, have a look at our [retrieval](basic_retrieval) tutorial.\n",
        "\n",
        "In this tutorial, we're going to:\n",
        "\n",
        "1. Get our data and split it into a training and test set.\n",
        "2. Implement a ranking model.\n",
        "3. Fit and evaluate it.\n",
        "\n",
        "\n",
        "## Imports\n",
        "\n",
        "\n",
        "Let's first get our imports out of the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gG3jLOGbaUv"
      },
      "outputs": [],
      "source": [
        "#!pip install -q tensorflow-recommenders\n",
        "#!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install tensorflow==2.15\n"
      ],
      "metadata": {
        "id": "40JbH2ZuZrR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install recommenders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yr8qAWkvnkc",
        "outputId": "6a0bfe65-9932-4b8b-c8d6-bee57cb72f55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recommenders\n",
            "  Downloading recommenders-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting category-encoders<3,>=2.6.0 (from recommenders)\n",
            "  Downloading category_encoders-2.8.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting cornac<3,>=1.15.2 (from recommenders)\n",
            "  Downloading cornac-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: hyperopt<1,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.2.7)\n",
            "Requirement already satisfied: lightgbm<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (4.5.0)\n",
            "Collecting locust<3,>=2.12.2 (from recommenders)\n",
            "  Downloading locust-2.32.8-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting memory-profiler<1,>=0.61.0 (from recommenders)\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: nltk<4,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from recommenders) (3.9.1)\n",
            "Requirement already satisfied: notebook<8,>=6.5.5 in /usr/local/lib/python3.11/dist-packages (from recommenders) (6.5.5)\n",
            "Requirement already satisfied: numba<1,>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.61.0)\n",
            "Requirement already satisfied: pandas<3.0.0,>2.0.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (2.2.2)\n",
            "Collecting retrying<2,>=1.3.4 (from recommenders)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (1.6.1)\n",
            "Collecting scikit-surprise>=1.1.3 (from recommenders)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seaborn<1,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.13.2)\n",
            "Requirement already satisfied: transformers<5,>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (4.48.2)\n",
            "Collecting pandera>=0.15.0 (from pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading pandera-0.22.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.4 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.14.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (1.26.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cornac<3,>=1.15.2->recommenders) (4.67.1)\n",
            "Collecting powerlaw (from cornac<3,>=1.15.2->recommenders)\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (0.10.9.7)\n",
            "Collecting ConfigArgParse>=1.5.5 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting Flask-Cors>=3.0.10 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting Flask-Login>=0.6.3 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Flask_Login-0.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (3.1.3)\n",
            "Requirement already satisfied: flask>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (3.1.0)\n",
            "Collecting gevent>=22.10.2 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading gevent-24.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting geventhttpclient>=2.3.1 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading geventhttpclient-2.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (5.9.5)\n",
            "Collecting pyzmq>=25.0.0 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading pyzmq-26.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (75.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.8.1->recommenders) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.8.1->recommenders) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.8.1->recommenders) (2024.11.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (3.1.5)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (6.4.2)\n",
            "INFO: pip is looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting notebook<8,>=6.5.5 (from recommenders)\n",
            "  Downloading notebook-7.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jupyterlab<4.4,>=4.3.4 (from notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (0.2.4)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<1,>=0.57.0->recommenders) (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2025.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (24.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (2.10.6)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (4.4.1)\n",
            "Collecting typing_inspect>=0.6.0 (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting hypothesis>=6.92.7 (from pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading hypothesis-6.125.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.2.0->recommenders) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn<1,>=0.13.0->recommenders) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.5.2)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (1.9.0)\n",
            "Collecting zope.event (from gevent>=22.10.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting zope.interface (from gevent>=22.10.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from gevent>=22.10.2->locust<3,>=2.12.2->recommenders) (3.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders) (2025.1.31)\n",
            "Collecting brotli (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders) (2.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers<5,>=4.27.0->recommenders) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers<5,>=4.27.0->recommenders) (4.12.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (25.1.0)\n",
            "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (23.1.0)\n",
            "Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (5.7.2)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.21.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (5.7.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (1.8.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.28.1)\n",
            "Collecting ipykernel>=6.5.0 (from jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=6.5.5->recommenders) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=6.5.5->recommenders) (4.23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->locust<3,>=2.12.2->recommenders) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->locust<3,>=2.12.2->recommenders) (3.10)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing_inspect>=0.6.0->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=2.0.0->locust<3,>=2.12.2->recommenders) (3.0.2)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from powerlaw->cornac<3,>=1.15.2->recommenders) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (2.27.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (21.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.14.0)\n",
            "Collecting comm>=0.1.1 (from ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (1.6.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=6.5.5->recommenders) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=6.5.5->recommenders) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=6.5.5->recommenders) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (4.3.6)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (2.18.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (1.4.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (4.9.0)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.4,>=4.3.4->notebook<8,>=6.5.5->recommenders) (0.2.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading recommenders-1.2.1-py3-none-any.whl (355 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.3/355.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.8.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cornac-2.3.0-cp311-cp311-manylinux1_x86_64.whl (25.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.4/25.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locust-2.32.8-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Downloading notebook-7.3.2-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandera-0.22.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Downloading Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
            "Downloading gevent-24.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geventhttpclient-2.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypothesis-6.125.2-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.7/480.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-26.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (874 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m874.5/874.5 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505162 sha256=3b34e60a0fb3cbbbfed972265e6146d369d0e800838c188bff4786e543e13533\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: sortedcontainers, brotli, zope.interface, zope.event, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, retrying, pyzmq, python-json-logger, overrides, mypy-extensions, memory-profiler, json5, jedi, hypothesis, fqdn, ConfigArgParse, comm, async-lru, typing_inspect, scikit-surprise, jupyter-server-terminals, jupyter-client, gevent, arrow, powerlaw, pandera, isoduration, ipykernel, geventhttpclient, Flask-Login, Flask-Cors, locust, cornac, category-encoders, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, recommenders\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 6.5.5\n",
            "    Uninstalling notebook-6.5.5:\n",
            "      Successfully uninstalled notebook-6.5.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 7.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ConfigArgParse-1.7 Flask-Cors-5.0.0 Flask-Login-0.6.3 arrow-1.3.0 async-lru-2.0.4 brotli-1.1.0 category-encoders-2.8.0 comm-0.2.2 cornac-2.3.0 fqdn-1.5.1 gevent-24.11.1 geventhttpclient-2.3.3 hypothesis-6.125.2 ipykernel-6.29.5 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-server-2.27.3 locust-2.32.8 memory-profiler-0.61.0 mypy-extensions-1.0.0 notebook-7.3.2 overrides-7.7.0 pandera-0.22.1 powerlaw-1.5 python-json-logger-3.2.1 pyzmq-26.2.1 recommenders-1.2.1 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scikit-surprise-1.1.4 sortedcontainers-2.4.0 types-python-dateutil-2.9.0.20241206 typing_inspect-0.9.0 uri-template-1.3.0 zope.event-5.0 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TODO:\n",
        "    - Instalar recommenders aqui a ver si puedo utilizarle las herramientas de medicion!.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Cg4aH6F3upFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from recommenders.evaluation.python_evaluation import (\n",
        "    map,\n",
        "    ndcg_at_k,\n",
        "    precision_at_k,\n",
        "    recall_at_k,\n",
        "    rmse,\n",
        "    mae,\n",
        "    logloss,\n",
        "    rsquared,\n",
        "    exp_var\n",
        ")"
      ],
      "metadata": {
        "id": "bIFMluhSwHT8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BxQ_hy7xPH3N"
      },
      "outputs": [],
      "source": [
        "#import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "klps6yrwaDzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9295c6be-4fc6-4e52-d281-07ea5d7b3466"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------\n",
        "#    base.py\n",
        "# ---------------\n",
        "\n",
        "# Copyright 2024 The TensorFlow Recommenders Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# lint-as: python3\n",
        "\n",
        "\"\"\"Base model.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "  \"\"\"Base model for TFRS models.\n",
        "\n",
        "  Many recommender models are relatively complex, and do not neatly fit into\n",
        "  supervised or unsupervised paradigms. This base class makes it easy to\n",
        "  define custom training and test losses for such complex models.\n",
        "\n",
        "  This is done by asking the user to implement the following methods:\n",
        "  - `__init__` to set up your model. Variable, task, loss, and metric\n",
        "    initialization should go here.\n",
        "  - `compute_loss` to define the training loss. The method takes as input the\n",
        "    raw features passed into the model, and returns a loss tensor for training.\n",
        "    As part of doing so, it should also update the model's metrics.\n",
        "  - [Optional] `call` to define how the model computes its predictions. This\n",
        "    is not always necessary: for example, two-tower retrieval models have two\n",
        "    well-defined submodels whose `call` methods are normally used directly.\n",
        "\n",
        "  Note that this base class is a thin conveniece wrapper for tf.keras.Model, and\n",
        "  equivalent functionality can easily be achieved by overriding the `train_step`\n",
        "  and `test_step` methods of a plain Keras model. Doing so also makes it easy\n",
        "  to build even more complex training mechanisms, such as the use of\n",
        "  different optimizers for different variables, or manipulating gradients.\n",
        "\n",
        "  Keras has an excellent tutorial on how to\n",
        "  do this [here](\n",
        "  https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_loss(self, inputs, training: bool = False) -> tf.Tensor:  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n",
        "    \"\"\"Defines the loss function.\n",
        "\n",
        "    Args:\n",
        "      inputs: A data structure of tensors: raw inputs to the model. These will\n",
        "        usually contain labels and weights as well as features.\n",
        "      training: Whether the model is in training mode.\n",
        "\n",
        "    Returns:\n",
        "      Loss tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    raise NotImplementedError(\n",
        "        \"Implementers must implement the `compute_loss` method.\")\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"Custom train step using the `compute_loss` method.\"\"\"\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = self.compute_loss(inputs, training=True)\n",
        "\n",
        "      # Handle regularization losses as well.\n",
        "      regularization_loss = tf.reduce_sum(\n",
        "          [tf.reduce_sum(loss) for loss in self.losses]\n",
        "      )\n",
        "\n",
        "      total_loss = loss + regularization_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
        "    metrics[\"loss\"] = loss\n",
        "    metrics[\"regularization_loss\"] = regularization_loss\n",
        "    metrics[\"total_loss\"] = total_loss\n",
        "\n",
        "    return metrics\n",
        "\n",
        "  def test_step(self, inputs):\n",
        "    \"\"\"Custom test step using the `compute_loss` method.\"\"\"\n",
        "\n",
        "    loss = self.compute_loss(inputs, training=False)\n",
        "\n",
        "    # Handle regularization losses as well.\n",
        "    regularization_loss = tf.reduce_sum(\n",
        "        [tf.reduce_sum(loss) for loss in self.losses]\n",
        "    )\n",
        "\n",
        "    total_loss = loss + regularization_loss\n",
        "\n",
        "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
        "    metrics[\"loss\"] = loss\n",
        "    metrics[\"regularization_loss\"] = regularization_loss\n",
        "    metrics[\"total_loss\"] = total_loss\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "skW4oNS1HEtB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------\n",
        "#    ranking.py\n",
        "# ---------------\n",
        "\n",
        "# Copyright 2024 The TensorFlow Recommenders Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Lint-as: python3\n",
        "\n",
        "\"\"\"A ranking task.\"\"\"\n",
        "\n",
        "from typing import List, Optional, Text\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Ranking(tf.keras.layers.Layer):\n",
        "  \"\"\"A ranking task.\n",
        "\n",
        "  Recommender systems are often composed of two components:\n",
        "  - a retrieval model, retrieving O(thousands) candidates from a corpus of\n",
        "    O(millions) candidates.\n",
        "  - a ranker model, scoring the candidates retrieved by the retrieval model to\n",
        "    return a ranked shortlist of a few dozen candidates.\n",
        "\n",
        "  This task helps with building ranker models. Usually, these will involve\n",
        "  predicting signals such as clicks, cart additions, likes, ratings, and\n",
        "  purchases.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      loss: Optional[tf.keras.losses.Loss] = None,\n",
        "      metrics: Optional[List[tf.keras.metrics.Metric]] = None,\n",
        "      prediction_metrics: Optional[List[tf.keras.metrics.Metric]] = None,\n",
        "      label_metrics: Optional[List[tf.keras.metrics.Metric]] = None,\n",
        "      loss_metrics: Optional[List[tf.keras.metrics.Metric]] = None,\n",
        "      name: Optional[Text] = None) -> None:\n",
        "    \"\"\"Initializes the task.\n",
        "\n",
        "    Args:\n",
        "      loss: Loss function. Defaults to BinaryCrossentropy.\n",
        "      metrics: List of Keras metrics to be evaluated.\n",
        "      prediction_metrics: List of Keras metrics used to summarize the\n",
        "        predictions.\n",
        "      label_metrics: List of Keras metrics used to summarize the labels.\n",
        "      loss_metrics: List of Keras metrics used to summarize the loss.\n",
        "      name: Optional task name.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._loss = (\n",
        "        loss if loss is not None else tf.keras.losses.BinaryCrossentropy())\n",
        "    self._ranking_metrics = metrics or []\n",
        "    self._prediction_metrics = prediction_metrics or []\n",
        "    self._label_metrics = label_metrics or []\n",
        "    self._loss_metrics = loss_metrics or []\n",
        "\n",
        "  def call(self,\n",
        "           labels: tf.Tensor,\n",
        "           predictions: tf.Tensor,\n",
        "           sample_weight: Optional[tf.Tensor] = None,\n",
        "           training: bool = False,\n",
        "           compute_metrics: bool = True) -> tf.Tensor:\n",
        "    \"\"\"Computes the task loss and metrics.\n",
        "\n",
        "    Args:\n",
        "      labels: Tensor of labels.\n",
        "      predictions: Tensor of predictions.\n",
        "      sample_weight: Tensor of sample weights.\n",
        "      training: Indicator whether training or test loss is being computed.\n",
        "      compute_metrics: Whether to compute metrics. Set this to False\n",
        "        during training for faster training.\n",
        "\n",
        "    Returns:\n",
        "      loss: Tensor of loss values.\n",
        "    \"\"\"\n",
        "\n",
        "    loss = self._loss(\n",
        "        y_true=labels, y_pred=predictions, sample_weight=sample_weight)\n",
        "\n",
        "    if not compute_metrics:\n",
        "      return loss\n",
        "\n",
        "    update_ops = []\n",
        "\n",
        "    for metric in self._ranking_metrics:\n",
        "      update_ops.append(metric.update_state(\n",
        "          y_true=labels, y_pred=predictions, sample_weight=sample_weight))\n",
        "\n",
        "    for metric in self._prediction_metrics:\n",
        "      update_ops.append(\n",
        "          metric.update_state(predictions, sample_weight=sample_weight))\n",
        "\n",
        "    for metric in self._label_metrics:\n",
        "      update_ops.append(\n",
        "          metric.update_state(labels, sample_weight=sample_weight))\n",
        "\n",
        "    for metric in self._loss_metrics:\n",
        "      update_ops.append(\n",
        "          metric.update_state(loss)\n",
        "      )  # Loss is a scalar here which is already weighted sum\n",
        "\n",
        "    # Custom metrics may not return update ops, unlike built-in\n",
        "    # Keras metrics.\n",
        "    update_ops = [x for x in update_ops if x is not None]\n",
        "\n",
        "    with tf.control_dependencies(update_ops):\n",
        "      return tf.identity(loss)\n"
      ],
      "metadata": {
        "id": "dachCpO8HYiJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "We're going to use the same data as the [retrieval](basic_retrieval) tutorial. This time, we're also going to keep the ratings: these are the objectives we are trying to predict."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")"
      ],
      "metadata": {
        "id": "fjqIGQG04R8k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aaQhqcLGP0jL"
      },
      "outputs": [],
      "source": [
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "a16nF36v5117",
        "outputId": "fa0ea27a-4e39-4366-8607-e3a35fb96d71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.map_op._MapDataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.data.ops.map_op._MapDataset</b><br/>def __init__(input_dataset, map_func, force_synchronous=False, use_inter_op_parallelism=True, preserve_cardinality=True, use_legacy_function=False, name=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py</a>A `Dataset` that maps a function over elements in its input.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 141);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfzAKrFF57-a",
        "outputId": "ef751e72-7e41-407f-897d-e2889a207586"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec={'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None)}>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu4XSa_G1nyN"
      },
      "source": [
        "As before, we'll split the data by putting 80% of the ratings in the train set, and 20% in the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)"
      ],
      "metadata": {
        "id": "NnV3uXa74JBB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7CC6QTd4LbA",
        "outputId": "f4789bdd-b28f-4028-906a-b7003fe2d15e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_ShuffleDataset element_spec={'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None)}>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = shuffled.take(80_000)"
      ],
      "metadata": {
        "id": "QAobj-9m6BVI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rS0eDfkjnjJL"
      },
      "outputs": [],
      "source": [
        "test = shuffled.skip(80_000).take(20_000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLSeQxaawrbP",
        "outputId": "70c45d64-f2a5-4dcf-e0d0-ad395fa413dd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec={'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None)}>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVi1HJfR9D7H"
      },
      "source": [
        "Let's also figure out unique user ids and movie titles present in the data.\n",
        "\n",
        "This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_titles = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])"
      ],
      "metadata": {
        "id": "UsZyalsb6OsW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_titles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Juay9Zj5xV-d",
        "outputId": "b6a2749b-2beb-41e6-dcb8-c7c7af25bc47"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])"
      ],
      "metadata": {
        "id": "poXDw-T06NUT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))"
      ],
      "metadata": {
        "id": "0un9It6r6QIR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MKROCiPo_5LJ"
      },
      "outputs": [],
      "source": [
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_movie_titles)"
      ],
      "metadata": {
        "id": "NPfxOBytC_Ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83902fe4-804c-4ba1-ba80-ba468fa886d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1664"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_user_ids)"
      ],
      "metadata": {
        "id": "rx2f-OuxDDkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274a44bb-f455-4d13-b297-32e3b38b5721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "943"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Vj9nHb48pn"
      },
      "source": [
        "## Implementing a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "### Architecture\n",
        "\n",
        "Ranking models do not face the same efficiency constraints as retrieval models do, and so we have a little bit more freedom in our choice of architectures.\n",
        "\n",
        "A model composed of multiple stacked dense layers is a relatively common architecture for ranking tasks. We can implement it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAk0y0Yf1eGh"
      },
      "outputs": [],
      "source": [
        "class RankingModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # Compute embeddings for users.\n",
        "    self.user_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for movies.\n",
        "    self.movie_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute predictions.\n",
        "    self.ratings = tf.keras.Sequential([\n",
        "      # Learn multiple dense layers.\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "      # Make rating predictions in the final layer.\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    user_id, movie_title = inputs\n",
        "\n",
        "    user_embedding = self.user_embeddings(user_id)\n",
        "    movie_embedding = self.movie_embeddings(movie_title)\n",
        "\n",
        "    return self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: DIBUJAR EL MODELO"
      ],
      "metadata": {
        "id": "UpHQT64b6qIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g76wZt-s2WmS"
      },
      "source": [
        "This model takes user ids and movie titles, and outputs a predicted rating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVxiAsRE2I8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795a6d78-62f2-45c8-c74a-8e9be411dc9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00655902]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "RankingModel()(([\"42\"], [\"One Flew Over the Cuckoo's Nest (1975)\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaCqJsXSkCo"
      },
      "source": [
        "### Loss and metrics\n",
        "\n",
        "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
        "\n",
        "In this instance, we'll make use of the `Ranking` task object: a convenience wrapper that bundles together the loss function and metric computation.\n",
        "\n",
        "We'll use it together with the `MeanSquaredError` Keras loss in order to predict the ratings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TODO:\n",
        "- Mirar que onda con ese tfrs.task.Ranking (Creo que tambien lo usamos), me gustaria saber que hace.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bNFNtnqg68CS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01a784ca-7707-470c-9f6b-78b92852feca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTODO:\\n- Mirar que onda con ese tfrs.task.Ranking (Creo que tambien lo usamos), me gustaria saber que hace.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ61Iz2QTBw3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18b0c51f-0cca-4a9f-9962-3039954185a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntask = tfrs.tasks.Ranking(\\n  loss = tf.keras.losses.MeanSquaredError(),\\n  metrics=[tf.keras.metrics.RootMeanSquaredError()]\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "\"\"\"\n",
        "task = tfrs.tasks.Ranking(\n",
        "  loss = tf.keras.losses.MeanSquaredError(),\n",
        "  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "task = Ranking(\n",
        "  loss = tf.keras.losses.MeanSquaredError(),\n",
        "  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_HwaCZa9HxPw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4fed21e-3849-4a64-b45d-cfe2befbc035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntask = Ranking(\\n  loss = tf.keras.losses.MeanSquaredError(),\\n  metrics=[tf.keras.metrics.RootMeanSquaredError()]\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3xFC-1cbz0"
      },
      "source": [
        "The task itself is a Keras layer that takes true and predicted as arguments, and returns the computed loss. We'll use that to implement the model's training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZUFeSlWRHGx"
      },
      "source": [
        "### The full model\n",
        "\n",
        "We can now put it all together into a model. TFRS exposes a base model class (`tfrs.models.Model`) which streamlines bulding models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
        "\n",
        "The base model will then take care of creating the appropriate training loop to fit our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n7c5CHFp0ow"
      },
      "outputs": [],
      "source": [
        "#class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "class MovielensModel(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ranking_model: tf.keras.Model = RankingModel()\n",
        "    \"\"\"\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "      loss = tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "    \"\"\"\n",
        "    self.task: tf.keras.layers.Layer = Ranking(\n",
        "      loss = tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "\n",
        "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
        "    return self.ranking_model(\n",
        "        (features[\"user_id\"], features[\"movie_title\"]))\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    labels = features.pop(\"user_rating\")\n",
        "\n",
        "    rating_predictions = self(features)\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(labels=labels, predictions=rating_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN_LJGlnRGo"
      },
      "source": [
        "## Fitting and evaluating\n",
        "\n",
        "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
        "\n",
        "Let's first instantiate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW63YaqP2wCf"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma0vc2XdN5g"
      },
      "source": [
        "Then shuffle, batch, and cache the training and evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53QJwY1gUnfv"
      },
      "outputs": [],
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8mHTxKAdTJO"
      },
      "source": [
        "Then train the  model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxPntlT8EFOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f736cc88-7c67-4d44-b029-a74567725bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "10/10 [==============================] - 7s 150ms/step - root_mean_squared_error: 2.0881 - loss: 4.0167 - regularization_loss: 0.0000e+00 - total_loss: 4.0167\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 1s 54ms/step - root_mean_squared_error: 1.1272 - loss: 1.2724 - regularization_loss: 0.0000e+00 - total_loss: 1.2724\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 1s 53ms/step - root_mean_squared_error: 1.1188 - loss: 1.2531 - regularization_loss: 0.0000e+00 - total_loss: 1.2531\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7eeb473b0610>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsluR8audV9W"
      },
      "source": [
        "As the model trains, the loss is falling and the RMSE metric is improving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gxp5RLFcv64"
      },
      "source": [
        "Finally, we can evaluate our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-zu6HLODNeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e66a43-16aa-49f0-d6ae-a638c1b3e422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 4s 31ms/step - root_mean_squared_error: 1.1109 - loss: 1.2295 - regularization_loss: 0.0000e+00 - total_loss: 1.2295\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'root_mean_squared_error': 1.1108818054199219,\n",
              " 'loss': 1.209473729133606,\n",
              " 'regularization_loss': 0.0,\n",
              " 'total_loss': 1.209473729133606}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKZyP9A1dxit"
      },
      "source": [
        "The lower the RMSE metric, the more accurate our model is at predicting ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcK4WKmKTE3A"
      },
      "source": [
        "## Testing the ranking model\n",
        "\n",
        "Now we can test the ranking model by computing predictions for a set of movies and then rank these movies based on the predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oB5DzrsTTrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97de82fe-afe1-473e-bf06-fb16b91da9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings:\n",
            "M*A*S*H (1970): [[3.4290922]]\n",
            "Dances with Wolves (1990): [[3.409481]]\n",
            "Speed (1994): [[3.3466034]]\n"
          ]
        }
      ],
      "source": [
        "test_ratings = {}\n",
        "test_movie_titles = [\"M*A*S*H (1970)\", \"Dances with Wolves (1990)\", \"Speed (1994)\"]\n",
        "for movie_title in test_movie_titles:\n",
        "  test_ratings[movie_title] = model({\n",
        "      \"user_id\": np.array([\"50\"]),\n",
        "      \"movie_title\": np.array([movie_title])\n",
        "  })\n",
        "\n",
        "print(\"Ratings:\")\n",
        "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f\"{title}: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Evaluar con metricas de recommenders.\n",
        "\n",
        "Seguir mi trabajo aqui\n"
      ],
      "metadata": {
        "id": "6bU1Io-bwY6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: - Para todos los usuarios en test, hacerle prediccion con todas las movies y rankearlas.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "J8q6gZUfuLBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TODO: - HACER PREDICCION A USUARIOS DE TEST\n",
        "    - Para todos los usuarios en test, hacerle prediccion con todas las movies y Rankearlas.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J7MEGFrZuaRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pensamiento:\n",
        "- Tengo que llegar a tener algo como recommenders de tipo\n",
        "\n",
        "\tuserID\titemID\tprediction\n",
        "0\t1\t433\t2.910697\n",
        "1\t1\t204\t2.906224\n",
        "2\t1\t403\t2.906136\n",
        "3\t1\t174\t2.870639\n",
        "4\t1\t70\t2.863253\n",
        "5\t1\t4\t2.854544\n",
        "6\t1\t98\t2.845376\n",
        "7\t1\t144\t2.817695\n",
        "8\t1\t423\t2.816989\n",
        "9\t1\t228\t2.813345\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I_rVhTfQtRBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    TODO: - Objetivo: Sacarle METRCAS a esto. (Utilizando recommmenders utils)\n",
        "        -  Utilizar las herramientas de medicion de recommenders para SACARLE METRICAS a esto!.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Z04FFu4TuiLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfedFnhBZiGw"
      },
      "source": [
        "## Exporting for serving\n",
        "\n",
        "The model can be easily exported for serving:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjLDKn5VZqm8"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(model, \"export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sia3ezFPZy1v"
      },
      "source": [
        "We can now load it back and perform predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owetAuj0Z1ny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f583e448-d012-4d0a-e2f3-27e27d44850d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.400573]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "loaded = tf.saved_model.load(\"export\")\n",
        "\n",
        "loaded({\"user_id\": np.array([\"42\"]), \"movie_title\": [\"Speed (1994)\"]}).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwj7cOOwjGzO"
      },
      "source": [
        "## Convert the model to TensorFLow Lite\n",
        "\n",
        "Although TensorFlow Recommenders is primarily designed to perform server-side recommendations, you can still convert the trained ranking model to TensorFLow Lite and run it on-device (for better user privacy privacy and lower latency).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEDKKtuQjDb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04782fd-b14e-4250-af5a-5f1c24ab69f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544480"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"export\")\n",
        "tflite_model = converter.convert()\n",
        "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLbzmVMLjuIy"
      },
      "source": [
        "Once the model is converted, you can run it like regular TensorFlow Lite models. Please check out [TensorFlow Lite documentation](https://www.tensorflow.org/lite/guide/inference) to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RASq6xAvjEiH"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model.\n",
        "if input_details[0][\"name\"] == \"serving_default_movie_title:0\":\n",
        "  interpreter.set_tensor(input_details[0][\"index\"], np.array([\"Speed (1994)\"]))\n",
        "  interpreter.set_tensor(input_details[1][\"index\"], np.array([\"42\"]))\n",
        "else:\n",
        "  interpreter.set_tensor(input_details[0][\"index\"], np.array([\"42\"]))\n",
        "  interpreter.set_tensor(input_details[1][\"index\"], np.array([\"Speed (1994)\"]))\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "rating = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efApI0Ii6srB"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "The model above gives us a decent start towards building a ranking system.\n",
        "\n",
        "Of course, making a practical ranking system requires much more effort.\n",
        "\n",
        "In most cases, a ranking model can be substantially improved by using more features rather than just user and candidate identifiers. To see how to do that, have a look at the [side features](featurization) tutorial.\n",
        "\n",
        "A careful understanding of the objectives worth optimizing is also necessary. To get started on building a recommender that optimizes multiple objectives, have a look at our [multitask](multitask) tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}